model_config:
    pretrained_model_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct
    device_map: 0
    # load_in_8bit: True
    attn_implementation: "flash_attention_2"
generation_config:
  max_new_tokens: 500

  # Beam search params
  num_beams: 5           # Number of beams to consider
  num_beam_groups: 5        # Number of beam groups (tries to encourage diversity between groups). Must be leq num_beams
  num_return_sequences: 5   # Number of samples to return. Must be leq num_beams
  diversity_penalty: 1.0    # Subtracted from a beamâ€™s score if it generates a token same as any beam from other group at a particular time

  # Sample params
  do_sample: False          # Whether to use greedy decoding. Must be False for beam search
  temperature: 1.0
  top_p: 1.0
  repetition_penalty : 1.0  # 1.0 means no penalty
sys_prompt: "You are a helpful assistant. Please respond in a helpful manner."